{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/s23170/GS2DIT/blob/main/gpt_2_shakespeare.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Source: \n",
        "\n",
        "*   https://pypi.org/project/gpt-2-simple/#description\n",
        "*   https://medium.com/@stasinopoulos.dimitrios/a-beginners-guide-to-training-and-generating-text-using-gpt2-c2f2e1fbd10a\n",
        "*   https://colab.research.google.com/drive/1VLG8e7YSEwypxU-noRNhsv5dW4NfTGce#scrollTo=VHdTL8NDbAh3\n",
        "*  https://github.com/ak9250/gpt-2-colab\n",
        "*  https://www.aiweirdness.com/d-and-d-character-bios-now-making-19-03-15/\n",
        "*  https://minimaxir.com/2019/09/howto-gpt2/\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rgNM-NcAZ9aT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/zawemi/GS2DIT/blob/main/Class%203/gpt_2_shakespeare.ipynb#scrollTo=4tIUvFbLMUuE)"
      ],
      "metadata": {
        "id": "4tIUvFbLMUuE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Let's teach AI writing like a Shakespeare ðŸŽ“"
      ],
      "metadata": {
        "id": "MofLJqBHAWXI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Installing the model"
      ],
      "metadata": {
        "id": "W7wiPFGQQn9o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQACJ8lyUIR0",
        "outputId": "66cfd982-6eb1-41c7-b12e-1e19182ccb9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gpt-2-simple\n",
            "  Downloading gpt_2_simple-0.8.1.tar.gz (26 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tensorflow>=2.5.1 in /usr/local/lib/python3.9/dist-packages (from gpt-2-simple) (2.11.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.9/dist-packages (from gpt-2-simple) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from gpt-2-simple) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from gpt-2-simple) (4.65.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from gpt-2-simple) (1.22.4)\n",
            "Collecting toposort\n",
            "  Downloading toposort-1.10-py3-none-any.whl (8.5 kB)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (3.19.6)\n",
            "Requirement already satisfied: tensorboard<2.12,>=2.11 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (2.11.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.51.3)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.6.3)\n",
            "Requirement already satisfied: keras<2.12,>=2.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (2.11.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (3.3.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.16.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (2.11.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (23.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (2.2.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (0.2.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.15.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (15.0.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (4.5.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (0.31.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (63.4.3)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (3.8.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (0.4.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (23.3.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->gpt-2-simple) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->gpt-2-simple) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->gpt-2-simple) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->gpt-2-simple) (2.0.12)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse>=1.6.0->tensorflow>=2.5.1->gpt-2-simple) (0.40.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (1.8.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (3.4.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (0.6.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (0.4.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (2.16.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (2.2.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (5.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (6.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (2.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (3.15.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow>=2.5.1->gpt-2-simple) (3.2.2)\n",
            "Building wheels for collected packages: gpt-2-simple\n",
            "  Building wheel for gpt-2-simple (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gpt-2-simple: filename=gpt_2_simple-0.8.1-py3-none-any.whl size=24576 sha256=d4cb5e478bdf44ad2e188a997a42fecb33c40a759a4edf591100189b2904fd77\n",
            "  Stored in directory: /root/.cache/pip/wheels/4e/28/f0/2f12e470be10d6804b193e4193d274c88995010fae512a67cf\n",
            "Successfully built gpt-2-simple\n",
            "Installing collected packages: toposort, gpt-2-simple\n",
            "Successfully installed gpt-2-simple-0.8.1 toposort-1.10\n"
          ]
        }
      ],
      "source": [
        "#install the library we'll use today\n",
        "!pip install gpt-2-simple"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Generating text with basic model"
      ],
      "metadata": {
        "id": "ADzeFwzaQ8cT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Importing and loading necessary components"
      ],
      "metadata": {
        "id": "d6Ah3D1CRK6o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import what we need\n",
        "import gpt_2_simple as gpt2 #for gpt-2 (our AI model)\n",
        "import os #lets us doing things with files and folders\n",
        "import requests #this one helps to dowload from the internet"
      ],
      "metadata": {
        "id": "mLg4pTPDaJJV"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#and let's download our AI model\n",
        "gpt2.download_gpt2()   # model is saved into current directory under /models/124M/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fIXHjaxvaWsV",
        "outputId": "f4973ec7-e896-4ae1-c3b0-4f6cb75717a0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching checkpoint: 1.05Mit [00:00, 487Mit/s]                                                      \n",
            "Fetching encoder.json: 1.05Mit [00:06, 155kit/s]\n",
            "Fetching hparams.json: 1.05Mit [00:00, 872Mit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 498Mit [01:12, 6.85Mit/s]\n",
            "Fetching model.ckpt.index: 1.05Mit [00:00, 457Mit/s]                                                \n",
            "Fetching model.ckpt.meta: 1.05Mit [00:01, 842kit/s]\n",
            "Fetching vocab.bpe: 1.05Mit [00:01, 816kit/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#strating the session so we can play with the gpt-2 model\n",
        "sess = gpt2.start_tf_sess()"
      ],
      "metadata": {
        "id": "6CCkn75KbBpg"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#we load the model from file to use it\n",
        "gpt2.load_gpt2(sess, run_name='124M', checkpoint_dir='models')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsBvHQsxZsyP",
        "outputId": "b97829de-b2ff-4fcb-8d62-f60727922aca"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading checkpoint models/124M/model.ckpt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Text generation"
      ],
      "metadata": {
        "id": "mDSFDj78RQJg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#this is how we would start model statement\n",
        "prefix = \"Is there a second Earth?\""
      ],
      "metadata": {
        "id": "-P5_fxZOgGlk"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#the model is generating text\n",
        "gpt2.generate(sess, run_name='124M', checkpoint_dir='models', prefix=prefix, length=50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSYqTat0gNDo",
        "outputId": "7290f859-307d-4752-aecc-5ad686173b3b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Is there a second Earth? Is there a second planet?\"\n",
            "\n",
            "\"Oh, but I can't say; I've heard there's a second Earth. One that lives on another planet. But there's a second planet somewhere. So I can't say. Can I?\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Generating text with improved (finetuned) model"
      ],
      "metadata": {
        "id": "ML5helfmRjT0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IMPORTANT**\n",
        "</br>Restart the runtime (Runtime -> Restart runtime)"
      ],
      "metadata": {
        "id": "8cEaZKtRPx0S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Importing and loading necessary components"
      ],
      "metadata": {
        "id": "NIPDKskeR7i3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import what we need\n",
        "import gpt_2_simple as gpt2 #for gpt-2 (our AI model)\n",
        "import os #lets us doing things with files and folders\n",
        "import requests #this one helps to dowload from the internet"
      ],
      "metadata": {
        "id": "eHys5-bWPnhJ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get nietzsche texts\n",
        "!wget \"https://s3.amazonaws.com/text-datasets/nietzsche.txt\""
      ],
      "metadata": {
        "id": "dRTQyR7IqaOl",
        "outputId": "a2bc2a52-6ebe-43a6-aa00-92f759e5a954",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-22 13:10:02--  https://s3.amazonaws.com/text-datasets/nietzsche.txt\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.139.21, 52.216.146.29, 54.231.135.80, ...\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.139.21|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 600901 (587K) [text/plain]\n",
            "Saving to: â€˜nietzsche.txt.1â€™\n",
            "\n",
            "nietzsche.txt.1     100%[===================>] 586.82K   538KB/s    in 1.1s    \n",
            "\n",
            "2023-03-22 13:10:04 (538 KB/s) - â€˜nietzsche.txt.1â€™ saved [600901/600901]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#game of thrones from https://www.kaggle.com/datasets/khulasasndh/game-of-thrones-books?select=001ssb.txt\n",
        "!gdown \"1CrL1wde_NGO68i5Prd_UNA_oW0cGQsxg&confirm=t\"\n",
        "!mv /content/001ssb.txt /content/got1.txt"
      ],
      "metadata": {
        "id": "pzDNTjJzuKDW",
        "outputId": "206cd359-6a29-480a-da54-068c18b4c5c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1CrL1wde_NGO68i5Prd_UNA_oW0cGQsxg&confirm=t\n",
            "To: /content/001ssb.txt\n",
            "\r  0% 0.00/1.63M [00:00<?, ?B/s]\r100% 1.63M/1.63M [00:00<00:00, 41.0MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#let's dowload a file with all Shakespeare plays\n",
        "!wget \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "!mv /content/input.txt /content/shakespeare.txt"
      ],
      "metadata": {
        "id": "9pwWGn5eqBJn",
        "outputId": "9ca157ac-be5c-4fe4-a67f-f9834a3c99a9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-22 13:10:13--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: â€˜input.txtâ€™\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.006s  \n",
            "\n",
            "2023-03-22 13:10:13 (185 MB/s) - â€˜input.txtâ€™ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#strating the session so we can play with the gpt-2 model\n",
        "sess = gpt2.start_tf_sess()"
      ],
      "metadata": {
        "id": "A0T2s8RxPnVr"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Teaching our model"
      ],
      "metadata": {
        "id": "bvllQvFxR9z6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#finetuning with shakespeare.txt (which, to be honest, means that we are teaching the model how to write like a shakespeare)\n",
        "#it takes a lot of time (~15min)...\n",
        "gpt2.finetune(sess, 'nietzsche.txt', steps=500)   # steps is max number of training steps"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2RJetxF6UOfY",
        "outputId": "10d8ee66-786e-4de6-cb12-9d2052f92286"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading checkpoint checkpoint/run1/model-500\n",
            "Loading dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset has 143770 tokens\n",
            "Training...\n",
            "======== SAMPLE 1 ========\n",
            " why was they? \n",
            "Page 244\n",
            "\n",
            "How did Khal Drogo come to be, Ootho wondered. It seemed as though they had arrived three years before, \n",
            "one before, the others after. The only hint of dawn or dusk was still a faint wisps of moonlight in the \n",
            "trunk, on their brow where the white of day was gone out. \n",
            "\"There are clansmen from the Shadow Lands in this mountain world,\" he said as they crept into \n",
            "the camp. \"My khal will have them at once.\" Across the godswood there was a long trestle stone stair, \n",
            "youths only; beyond it a huge tripod raised in the shape of a lion, with towers of twisted leaves off to the \n",
            "left of the tripod's center. \n",
            "Dany turned in a different direction, a strange look on her face. \"Where is your mother?\" she asked after \n",
            "her prince. \n",
            "\"She's gone, old lady,\" Drogo answered, \"and my father's not here. I was with the clansmen last night. \n",
            "They rode together, I think.\" \n",
            "The clansmen were quiet for a brief moment. Dany thought they knew where her father was \n",
            "remarking; a long day ahead of their arrival, when she would have most like ridden off to spend the night. \n",
            "\"There's no moon,\" she said as she rode past a tall brown bear, its eyes red and red with \n",
            "hunger. The beast had the long face, its mouth twisted in a frown the sound of which frightened \n",
            "her. To its ears it glinted dark red, its cheeks reddened by the burning of oil on its skin. \n",
            "Dany remembered the story she had told her sister, the tale of a terrible girl lost in the forests. The direwolf \n",
            "had told her one night as they rode through a wood and a stream, when a strange wolf emerged from under the tree \n",
            "and began to talk. Howling was a lesson for every wolfswood, whatever name they might name the creature. \n",
            "Dany could remember the first time she rode out with her sister. The forest was a hundred feet higher than she \n",
            "could carry her tent and wheel her horse, and she could see the faces of everyone: the pack wolves, the yellowed, \n",
            "necked redhead packs, the greyhair blue-eyed wagons full of mischief, the girls shouting their favorites. \n",
            "The first time she rode out with her sister, Khal Drogo had not shown her. The Greatjon had come with \n",
            "her on her mount, and Dany had gone with him on her girdle. That had been a rocky, twisting, and uncomfortable ride. \n",
            "Dany remembered hearing shouts from the camp as the first of the red wolves moved through the trees as Rakharo \n",
            "Page 245\n",
            "\n",
            "and the other men ran off with the bear. The men had been of the Shaggydog blood, and blood of Drogo's, neither \n",
            "Page 246\n",
            "\n",
            "nor of Shaggydog. She must have heard the names at once, and Dany knew it was the same woman she \n",
            "heard: Mirri Maz Duur. \n",
            "\"No one knows who I may have been,\" she said as the pack of redwoods burst from under her gauntlet and \n",
            "crashed into the stream where they had come. She saw a man cut a man in her khalasar. The crowd moved aside. \n",
            "The rest of the forest was green and flat, aside from where two of the black ironwoods seemed to be, yet \n",
            "it did not make them vanish. Dany saw a great antlered antlered, a monstrous brown thing with a great antlered \n",
            "head and a hundred heads. She tried and failed to get up from the tree; the trees snapped under the weight of \n",
            "the antlered. Even on the mountain, the wind was strong in the antlered, and somehow she had \n",
            "never lost control over her mount. The saddling harness snapped at her wrists, and her legs took fright. The straps \n",
            "seemed to snap themselves, \n",
            "Dany saw. She struggled to get away, never so much as flap her legs to help herself. None of her \n",
            "straps seemed to free her. Her arms were sore, and her gums were starting to blister. She lurched \n",
            "back and forth between them, failing both times. In the jaws of the second fall, Drogo's wolf tore the man \n",
            "in half, knocking him to the ground. \n",
            "The second fall was less so. The man had already died by that time, with his bloody feet still wet with \n",
            "water, and there was rustling on the ground like a wet brush. By then Drogo's blood had run from his\n",
            "\n",
            "[501 | 19.73] loss=5.89 avg=5.89\n",
            "[502 | 22.02] loss=4.67 avg=5.28\n",
            "[503 | 24.32] loss=4.34 avg=4.96\n",
            "[504 | 26.62] loss=4.36 avg=4.81\n",
            "[505 | 28.93] loss=4.26 avg=4.70\n",
            "[506 | 31.23] loss=4.01 avg=4.58\n",
            "[507 | 33.52] loss=4.04 avg=4.50\n",
            "[508 | 35.80] loss=3.95 avg=4.43\n",
            "[509 | 38.09] loss=4.12 avg=4.39\n",
            "[510 | 40.36] loss=3.93 avg=4.34\n",
            "[511 | 42.63] loss=3.90 avg=4.30\n",
            "[512 | 44.89] loss=3.97 avg=4.27\n",
            "[513 | 47.17] loss=3.86 avg=4.24\n",
            "[514 | 49.42] loss=3.90 avg=4.21\n",
            "[515 | 51.66] loss=3.94 avg=4.19\n",
            "[516 | 53.91] loss=3.94 avg=4.18\n",
            "[517 | 56.15] loss=3.81 avg=4.15\n",
            "[518 | 58.38] loss=3.80 avg=4.13\n",
            "[519 | 60.60] loss=3.74 avg=4.11\n",
            "[520 | 62.84] loss=3.69 avg=4.09\n",
            "[521 | 65.06] loss=3.85 avg=4.07\n",
            "[522 | 67.29] loss=3.76 avg=4.06\n",
            "[523 | 69.51] loss=3.72 avg=4.04\n",
            "[524 | 71.73] loss=3.61 avg=4.02\n",
            "[525 | 73.96] loss=3.72 avg=4.01\n",
            "[526 | 76.19] loss=3.62 avg=3.99\n",
            "[527 | 78.41] loss=3.86 avg=3.99\n",
            "[528 | 80.64] loss=3.76 avg=3.98\n",
            "[529 | 82.87] loss=3.65 avg=3.96\n",
            "[530 | 85.11] loss=3.63 avg=3.95\n",
            "[531 | 87.36] loss=3.65 avg=3.94\n",
            "[532 | 89.60] loss=3.72 avg=3.93\n",
            "[533 | 91.85] loss=3.74 avg=3.92\n",
            "[534 | 94.10] loss=3.67 avg=3.92\n",
            "[535 | 96.35] loss=3.63 avg=3.91\n",
            "[536 | 98.60] loss=3.61 avg=3.90\n",
            "[537 | 100.86] loss=3.49 avg=3.88\n",
            "[538 | 103.12] loss=3.63 avg=3.88\n",
            "[539 | 105.39] loss=3.50 avg=3.86\n",
            "[540 | 107.65] loss=3.74 avg=3.86\n",
            "[541 | 109.92] loss=3.63 avg=3.85\n",
            "[542 | 112.18] loss=3.60 avg=3.85\n",
            "[543 | 114.45] loss=3.61 avg=3.84\n",
            "[544 | 116.72] loss=3.64 avg=3.83\n",
            "[545 | 118.99] loss=3.43 avg=3.82\n",
            "[546 | 121.25] loss=3.45 avg=3.81\n",
            "[547 | 123.52] loss=3.58 avg=3.81\n",
            "[548 | 125.79] loss=3.41 avg=3.80\n",
            "[549 | 128.06] loss=3.41 avg=3.79\n",
            "[550 | 130.32] loss=3.55 avg=3.78\n",
            "[551 | 132.57] loss=3.27 avg=3.77\n",
            "[552 | 134.82] loss=3.47 avg=3.76\n",
            "[553 | 137.06] loss=3.41 avg=3.75\n",
            "[554 | 139.31] loss=3.45 avg=3.74\n",
            "[555 | 141.56] loss=3.44 avg=3.74\n",
            "[556 | 143.84] loss=3.43 avg=3.73\n",
            "[557 | 146.14] loss=3.48 avg=3.72\n",
            "[558 | 148.41] loss=3.16 avg=3.71\n",
            "[559 | 150.65] loss=3.59 avg=3.71\n",
            "[560 | 152.90] loss=3.51 avg=3.70\n",
            "[561 | 155.14] loss=3.46 avg=3.70\n",
            "[562 | 157.39] loss=3.33 avg=3.69\n",
            "[563 | 159.64] loss=3.41 avg=3.68\n",
            "[564 | 161.89] loss=3.30 avg=3.68\n",
            "[565 | 164.13] loss=3.36 avg=3.67\n",
            "[566 | 166.39] loss=3.01 avg=3.66\n",
            "[567 | 168.64] loss=3.43 avg=3.65\n",
            "[568 | 170.89] loss=3.51 avg=3.65\n",
            "[569 | 173.13] loss=3.36 avg=3.64\n",
            "[570 | 175.38] loss=3.51 avg=3.64\n",
            "[571 | 177.63] loss=3.44 avg=3.64\n",
            "[572 | 179.89] loss=3.44 avg=3.63\n",
            "[573 | 182.13] loss=3.17 avg=3.62\n",
            "[574 | 184.38] loss=3.13 avg=3.61\n",
            "[575 | 186.63] loss=3.22 avg=3.61\n",
            "[576 | 188.87] loss=3.23 avg=3.60\n",
            "[577 | 191.13] loss=3.39 avg=3.60\n",
            "[578 | 193.37] loss=3.38 avg=3.59\n",
            "[579 | 195.63] loss=3.21 avg=3.59\n",
            "[580 | 197.89] loss=3.53 avg=3.58\n",
            "[581 | 200.14] loss=3.20 avg=3.58\n",
            "[582 | 202.38] loss=3.21 avg=3.57\n",
            "[583 | 204.66] loss=3.14 avg=3.56\n",
            "[584 | 206.92] loss=3.43 avg=3.56\n",
            "[585 | 209.18] loss=3.12 avg=3.55\n",
            "[586 | 211.43] loss=3.37 avg=3.55\n",
            "[587 | 213.68] loss=3.25 avg=3.54\n",
            "[588 | 215.93] loss=3.13 avg=3.54\n",
            "[589 | 218.18] loss=3.20 avg=3.53\n",
            "[590 | 220.44] loss=3.30 avg=3.53\n",
            "[591 | 222.71] loss=3.35 avg=3.53\n",
            "[592 | 224.95] loss=3.20 avg=3.52\n",
            "[593 | 227.20] loss=3.07 avg=3.51\n",
            "[594 | 229.46] loss=3.16 avg=3.51\n",
            "[595 | 231.72] loss=3.10 avg=3.50\n",
            "[596 | 233.97] loss=2.91 avg=3.49\n",
            "[597 | 236.22] loss=3.25 avg=3.49\n",
            "[598 | 238.46] loss=3.24 avg=3.48\n",
            "[599 | 240.71] loss=2.92 avg=3.47\n",
            "[600 | 242.96] loss=3.23 avg=3.47\n",
            "======== SAMPLE 1 ========\n",
            " and as the mother of men of whom we will be led. (17)\n",
            "\n",
            "It is just as if a mother were to give birth in the garden of the god,\n",
            "and the son in heaven, and that the father might now be the father,\n",
            "to take place after the god has made him a god by his own actions and\n",
            "actions, as a god who ought to have a will and a power beyond the will,\n",
            "and who ought, by means of the will and the power, to be strong enough\n",
            "to overcome even a destructive will. One day, even the strongest,\n",
            "the most powerful will the strongest, an instinct of submission,\n",
            "of courage will prevail over the will the strongest. In the end, the will\n",
            "must be the will, but not the will alone, the will alone the will; the ego\n",
            "is the ego by its essence. In men of the egoism, who are not strong enough\n",
            "enough are strong enough, the strength of the egoism will lead them to\n",
            "discipline, to the formation of strong bonds that will keep the weak free;\n",
            "the ego will lead them to a knowledge of God and God is the essence.\n",
            "(18)\n",
            "\n",
            "CHAPTER III. The Religious World.--There is now a religious world\n",
            "in which the best and the worst of man are fused together. Thus a\n",
            "religion can be devised for the sole purpose of divining out the truth which the\n",
            "god has in fact made manifest--what does he have to say about the\n",
            "truth? He has spoken of himself as a god, but he has really\n",
            "made no mention of his own nature but only about the soul. He\n",
            "has spoken about himself only concerning the God, the soul, heaven and\n",
            "earth. There are others who have only one nature: they are the ones\n",
            "who lack a God in the one fundamental sense, or a God in the\n",
            "extent and not of the other fundamental senses--and it is not a God at all.\n",
            "Wherever any of a number of things appears in heaven as just appearances,\n",
            "that is not his religion--that is his religion as it were. In that\n",
            "wherever one knows that the religion of Hercules arose\n",
            "from the belief that there was a god but in the same\n",
            "cause. What does that make Hercules a religion? For for an instant,\n",
            "it must be admitted that Hercules was the actual creation. This is\n",
            "the \"holy delusion,\" from whence the origin of religions. This\n",
            "hypnotic world has its basis in the belief which every person of\n",
            "every age gives to religion and in the belief which is ascribed\n",
            "to any of the gods: not the god, but the existence of some other, the\n",
            "god is as well. (19)\n",
            "\n",
            "15. So, too, that is something which every great science has\n",
            "doubted. For there is an immense variety of ideas and\n",
            "antidextensions from the beginning. The one, the rational and the\n",
            "egoistic, all share in it. This seems a remarkable fact, the\n",
            "super-abundance of which, even in the most primitive religions, was\n",
            "as much the work of man in its inception, as his fundamental assumptions--or\n",
            "itself is his origin. This fact which has to be explained has only\n",
            "concurringly and honestly been kept in mind in the course of the\n",
            "philosophical history of the whole of mankind--in effect it\n",
            "has been brought down to earth. It is only because of this\n",
            "super-abundance, that science has in its whole history been\n",
            "unsuccessful in the development of which the scientific man is the principal\n",
            "sensory element, a condition of life that cannot be changed: it is a\n",
            "super-abundance. Scientific culture is a slavery of mankind, the\n",
            "super-abundance which can hardly be reduced to mere forms; it is\n",
            "a slavery which, in the midst of the super-abundance, makes men\n",
            "sick and ill-dispersed.\n",
            "\n",
            "16. To me and to the rest of the world, God is the only\n",
            "greatest and most exalted of all gods, and is the\n",
            "greatest treasure which I have seen. If I could understand what makes\n",
            "him the greatest and the most powerful man, how must men possibly understand\n",
            "why he is such a terrible thing?--why does he not take place in\n",
            "the midst of everything that has power to give him power? But how\n",
            "many philosophers can understand him? The most of them are\n",
            "only philosophers, and if I were to understand him more, my whole soul would\n",
            "appear to be filled with a vast stream of strange and\n",
            "dreadful thinking. (20)\n",
            "\n",
            "17. The most beautiful and magnificent things which a man finds as he\n",
            "is passing through a net: (20)\n",
            "\n",
            "17. To understand a human being as a thing, which has the\n",
            "same properties as man; and\n",
            "\n",
            "[601 | 256.64] loss=3.36 avg=3.47\n",
            "[602 | 258.88] loss=2.96 avg=3.46\n",
            "[603 | 261.13] loss=3.27 avg=3.46\n",
            "[604 | 263.38] loss=2.93 avg=3.45\n",
            "[605 | 265.63] loss=2.84 avg=3.44\n",
            "[606 | 267.87] loss=3.03 avg=3.43\n",
            "[607 | 270.12] loss=3.17 avg=3.43\n",
            "[608 | 272.37] loss=2.95 avg=3.42\n",
            "[609 | 274.61] loss=3.03 avg=3.42\n",
            "[610 | 276.86] loss=2.78 avg=3.41\n",
            "[611 | 279.10] loss=2.57 avg=3.39\n",
            "[612 | 281.35] loss=2.70 avg=3.38\n",
            "[613 | 283.60] loss=3.21 avg=3.38\n",
            "[614 | 285.85] loss=2.98 avg=3.38\n",
            "[615 | 288.10] loss=3.13 avg=3.37\n",
            "[616 | 290.34] loss=3.13 avg=3.37\n",
            "[617 | 292.59] loss=2.91 avg=3.36\n",
            "[618 | 294.83] loss=3.02 avg=3.36\n",
            "[619 | 297.08] loss=3.05 avg=3.35\n",
            "[620 | 299.33] loss=2.98 avg=3.35\n",
            "[621 | 301.58] loss=2.95 avg=3.34\n",
            "[622 | 303.83] loss=2.94 avg=3.34\n",
            "[623 | 306.08] loss=2.60 avg=3.33\n",
            "[624 | 308.33] loss=2.79 avg=3.32\n",
            "[625 | 310.58] loss=3.02 avg=3.31\n",
            "[626 | 312.83] loss=3.00 avg=3.31\n",
            "[627 | 315.08] loss=2.66 avg=3.30\n",
            "[628 | 317.33] loss=2.83 avg=3.29\n",
            "[629 | 319.58] loss=2.66 avg=3.29\n",
            "[630 | 321.82] loss=2.61 avg=3.28\n",
            "[631 | 324.07] loss=2.79 avg=3.27\n",
            "[632 | 326.32] loss=2.63 avg=3.26\n",
            "[633 | 328.57] loss=2.91 avg=3.26\n",
            "[634 | 330.82] loss=3.15 avg=3.25\n",
            "[635 | 333.06] loss=3.01 avg=3.25\n",
            "[636 | 335.31] loss=2.89 avg=3.25\n",
            "[637 | 337.55] loss=2.81 avg=3.24\n",
            "[638 | 339.80] loss=2.77 avg=3.23\n",
            "[639 | 342.05] loss=2.75 avg=3.23\n",
            "[640 | 344.31] loss=2.54 avg=3.22\n",
            "[641 | 346.56] loss=2.80 avg=3.21\n",
            "[642 | 348.81] loss=2.78 avg=3.21\n",
            "[643 | 351.06] loss=2.95 avg=3.20\n",
            "[644 | 353.31] loss=2.66 avg=3.20\n",
            "[645 | 355.55] loss=2.78 avg=3.19\n",
            "[646 | 357.80] loss=2.42 avg=3.18\n",
            "[647 | 360.04] loss=2.75 avg=3.18\n",
            "[648 | 362.29] loss=2.61 avg=3.17\n",
            "[649 | 364.54] loss=2.87 avg=3.16\n",
            "[650 | 366.80] loss=2.55 avg=3.16\n",
            "[651 | 369.05] loss=2.55 avg=3.15\n",
            "[652 | 371.30] loss=2.38 avg=3.14\n",
            "[653 | 373.56] loss=2.74 avg=3.13\n",
            "[654 | 375.80] loss=2.66 avg=3.13\n",
            "[655 | 378.06] loss=2.59 avg=3.12\n",
            "[656 | 380.31] loss=2.69 avg=3.12\n",
            "[657 | 382.57] loss=2.77 avg=3.11\n",
            "[658 | 384.82] loss=2.44 avg=3.10\n",
            "[659 | 387.07] loss=2.51 avg=3.10\n",
            "[660 | 389.32] loss=3.15 avg=3.10\n",
            "[661 | 391.58] loss=2.64 avg=3.09\n",
            "[662 | 393.83] loss=2.55 avg=3.08\n",
            "[663 | 396.08] loss=2.48 avg=3.08\n",
            "[664 | 398.33] loss=2.65 avg=3.07\n",
            "[665 | 400.58] loss=2.49 avg=3.06\n",
            "[666 | 402.84] loss=2.48 avg=3.06\n",
            "[667 | 405.09] loss=2.84 avg=3.05\n",
            "[668 | 407.33] loss=2.85 avg=3.05\n",
            "[669 | 409.58] loss=2.55 avg=3.05\n",
            "[670 | 411.83] loss=2.39 avg=3.04\n",
            "[671 | 414.08] loss=2.66 avg=3.03\n",
            "[672 | 416.34] loss=2.52 avg=3.03\n",
            "[673 | 418.59] loss=2.44 avg=3.02\n",
            "[674 | 420.84] loss=2.22 avg=3.01\n",
            "[675 | 423.09] loss=2.57 avg=3.00\n",
            "[676 | 425.34] loss=2.16 avg=2.99\n",
            "[677 | 427.60] loss=2.51 avg=2.99\n",
            "[678 | 429.84] loss=2.68 avg=2.98\n",
            "[679 | 432.09] loss=2.40 avg=2.98\n",
            "[680 | 434.34] loss=2.50 avg=2.97\n",
            "[681 | 436.60] loss=2.68 avg=2.97\n",
            "[682 | 438.86] loss=2.28 avg=2.96\n",
            "[683 | 441.11] loss=2.22 avg=2.95\n",
            "[684 | 443.38] loss=2.27 avg=2.94\n",
            "[685 | 445.63] loss=2.31 avg=2.94\n",
            "[686 | 447.88] loss=2.26 avg=2.93\n",
            "[687 | 450.13] loss=2.74 avg=2.93\n",
            "[688 | 452.38] loss=2.47 avg=2.92\n",
            "[689 | 454.62] loss=1.69 avg=2.91\n",
            "[690 | 456.88] loss=2.30 avg=2.90\n",
            "[691 | 459.13] loss=2.35 avg=2.89\n",
            "[692 | 461.39] loss=2.27 avg=2.88\n",
            "[693 | 463.64] loss=1.97 avg=2.87\n",
            "[694 | 465.90] loss=2.02 avg=2.86\n",
            "[695 | 468.15] loss=2.78 avg=2.86\n",
            "[696 | 470.40] loss=2.89 avg=2.86\n",
            "[697 | 472.65] loss=2.41 avg=2.86\n",
            "[698 | 474.90] loss=2.30 avg=2.85\n",
            "[699 | 477.15] loss=2.04 avg=2.84\n",
            "[700 | 479.39] loss=1.83 avg=2.83\n",
            "======== SAMPLE 1 ========\n",
            " of an immense multitude of gods. Here, however, I would go even further, to say\n",
            "expressly how I see the problem of Gods and Man in the domain of religions evolved and\n",
            "consequently developed, at the very period when the various religions were\n",
            "as a movement, as the scaffolding upon which mankind was raised, into an\n",
            "instinctive and rational equilibrium, and that in their highest forms--in their\n",
            "most creative strength--finally, the refined form of their conception, the\n",
            "long and muscular form of Kundalini and Asiatic motion, were the elements for\n",
            "generation, while the entire system of ideas and systems of faith, religion\n",
            "and theology, was the system of derivation and synthesis--I would,\n",
            "expressly, say so. That is why I have set the question: What is the\n",
            "domain of religions (Gods and Man) in accordance with the conditions referred\n",
            "to in the above-quoted text?\n",
            "\n",
            "269. The essence of morality is the utilitarian principle that if a utilitarian\n",
            "and non-impartial soul regards the world in accordance with its worth,\n",
            "towards the sake of value, and consequently does not perceive itself in\n",
            "value, then the utilitarian principle of worth is manifestly inadequate,\n",
            "for it requires and assumes utilitarian causation and causes. It can\n",
            "not be attained without a utilitarian synthesis that communicates the\n",
            "ultimate end of morality, for the synthetic morality of virtue itself\n",
            "must be its ultimate morality. A synthesis of the two most fundamental morality\n",
            "regards itself as this; that is to say, as the synthetic\n",
            "perception of itself, that is, as the conception of morality, is the\n",
            "foundation of it, and the conception of morality itself is the means by which\n",
            "\n",
            "ideal and utility are attained. In moral conceptions it is the necessity, necessity,\n",
            "necessity, that is, as to be necessary to the ends attained.\n",
            "Thus morality, by the aid of the synthetic conception thereof, stands in the\n",
            "very same relation as health and health by the aid of utilitarian causation, the\n",
            "consequence of the utilitarian principle, and also its effect upon the\n",
            "general equilibrium of the soul. \"Ego mali,\" says Kant, \"is utilitarian just as it\n",
            "is, mali mali veritas.\" But whoever has knowledge of the nature of moral causation,\n",
            "will agree that a malignant desire for knowledge, a desire for knowledge\n",
            "which is as irrational as knowledge of the cause of the causes of pain, a\n",
            "delicacy of malignity, which every soul desires madly, is evil, tyrannical, without\n",
            "reason and without justice, just as was formerly the case, just as knowledge\n",
            "of causation is unjust and unjust. On the other hand, whoever has the will\n",
            "and obtains for it, if, therefore, we have an immense variety of morality, we\n",
            "must have the same existence, so that every one will be free from prejudice,\n",
            "misconception, and oppression, as every other soul has been free from\n",
            " prejudice and oppression. \"Morality is the will to knowledge,\" says Kant. \"Knowledge is\n",
            "the will to knowledge, and the will is to the belief in knowledge, as the\n",
            "instincts, instincts, and wills.\" Here again Kant commands caution. \"Knowledge\n",
            "is our desire,\" he adds, \"and as far as we know not desires it does not\n",
            "conclusively require belief.\" Thus the malignity of moral sentiment, the seductive and\n",
            "beautiful interpretations of bad acts, is evidence of evil conduct hitherto experienced by\n",
            "us. The malignity of false injunctions, the evidence of moral sentiments, is not the\n",
            "only evidence of evil conduct hitherto experienced by us. We also perceive what is\n",
            "evidence of evil conduct, that is to say, an opinion which is bad, dangerous,\n",
            "totally unfair, unfair, and which the majority of the nations of the nations hear,\n",
            "thus convince them of the evil character of everything that is believed, of\n",
            "the evil character of everything that is undertaken out of diligence and caution. Indeed the\n",
            "majority will even say: \"I have forbidden everything that is done to us out of\n",
            "pleasure, out of longing, out of disgust, out of envy.\" It is an opinion which is the\n",
            "origin of unhappiness, an opinion which is the evidence of bad conduct hitherto experienced by\n",
            "us. It does not follow that this majority, all of them all moralists, also\n",
            "hypatisers, liars, scoundrels, plebeians, in every sense of the word which they\n",
            "ideally wish to employ in their judgments, but which have hitherto only been based on\n",
            "inconsecrated opinions and arbitrary laws, and which have for its object only a\n",
            "reluctance and disregard for, compulsion, and oppression--it is those opinions\n",
            "which they believe are the means and end in themselves, their justification is in the\n",
            "matter quite the opposite\n",
            "\n",
            "[701 | 492.68] loss=2.18 avg=2.82\n",
            "[702 | 494.93] loss=2.22 avg=2.82\n",
            "[703 | 497.18] loss=2.11 avg=2.81\n",
            "[704 | 499.44] loss=1.98 avg=2.80\n",
            "[705 | 501.69] loss=2.52 avg=2.80\n",
            "[706 | 503.94] loss=1.94 avg=2.79\n",
            "[707 | 506.19] loss=1.97 avg=2.78\n",
            "[708 | 508.44] loss=2.30 avg=2.77\n",
            "[709 | 510.69] loss=2.24 avg=2.77\n",
            "[710 | 512.94] loss=1.90 avg=2.76\n",
            "[711 | 515.19] loss=2.10 avg=2.75\n",
            "[712 | 517.44] loss=1.77 avg=2.74\n",
            "[713 | 519.69] loss=1.85 avg=2.73\n",
            "[714 | 521.94] loss=2.13 avg=2.72\n",
            "[715 | 524.20] loss=2.45 avg=2.72\n",
            "[716 | 526.45] loss=2.14 avg=2.71\n",
            "[717 | 528.69] loss=1.82 avg=2.70\n",
            "[718 | 530.94] loss=2.33 avg=2.70\n",
            "[719 | 533.19] loss=2.37 avg=2.69\n",
            "[720 | 535.44] loss=1.46 avg=2.68\n",
            "[721 | 537.68] loss=1.99 avg=2.67\n",
            "[722 | 539.93] loss=1.79 avg=2.66\n",
            "[723 | 542.17] loss=1.81 avg=2.65\n",
            "[724 | 544.42] loss=1.93 avg=2.64\n",
            "[725 | 546.67] loss=1.83 avg=2.63\n",
            "[726 | 548.92] loss=1.81 avg=2.63\n",
            "[727 | 551.17] loss=1.94 avg=2.62\n",
            "[728 | 553.42] loss=2.10 avg=2.61\n",
            "[729 | 555.66] loss=1.79 avg=2.60\n",
            "[730 | 557.91] loss=1.87 avg=2.59\n",
            "[731 | 560.16] loss=1.93 avg=2.59\n",
            "[732 | 562.40] loss=1.88 avg=2.58\n",
            "[733 | 564.65] loss=1.75 avg=2.57\n",
            "[734 | 566.90] loss=1.52 avg=2.56\n",
            "[735 | 569.15] loss=2.18 avg=2.55\n",
            "[736 | 571.40] loss=2.15 avg=2.55\n",
            "[737 | 573.65] loss=1.76 avg=2.54\n",
            "[738 | 575.90] loss=1.59 avg=2.53\n",
            "[739 | 578.15] loss=1.69 avg=2.52\n",
            "[740 | 580.39] loss=1.64 avg=2.51\n",
            "[741 | 582.65] loss=1.90 avg=2.51\n",
            "[742 | 584.91] loss=1.85 avg=2.50\n",
            "[743 | 587.16] loss=2.34 avg=2.50\n",
            "[744 | 589.41] loss=2.30 avg=2.49\n",
            "[745 | 591.66] loss=1.68 avg=2.49\n",
            "[746 | 593.91] loss=1.66 avg=2.48\n",
            "[747 | 596.16] loss=1.93 avg=2.47\n",
            "[748 | 598.40] loss=1.77 avg=2.46\n",
            "[749 | 600.65] loss=1.54 avg=2.45\n",
            "[750 | 602.90] loss=1.37 avg=2.44\n",
            "[751 | 605.15] loss=1.68 avg=2.43\n",
            "[752 | 607.40] loss=1.56 avg=2.42\n",
            "[753 | 609.64] loss=1.82 avg=2.42\n",
            "[754 | 611.89] loss=1.69 avg=2.41\n",
            "[755 | 614.14] loss=1.70 avg=2.40\n",
            "[756 | 616.38] loss=1.91 avg=2.40\n",
            "[757 | 618.63] loss=1.39 avg=2.38\n",
            "[758 | 620.88] loss=1.68 avg=2.38\n",
            "[759 | 623.12] loss=1.60 avg=2.37\n",
            "[760 | 625.37] loss=1.87 avg=2.36\n",
            "[761 | 627.61] loss=1.23 avg=2.35\n",
            "[762 | 629.86] loss=1.41 avg=2.34\n",
            "[763 | 632.11] loss=1.46 avg=2.33\n",
            "[764 | 634.36] loss=1.49 avg=2.32\n",
            "[765 | 636.60] loss=1.21 avg=2.31\n",
            "[766 | 638.85] loss=1.09 avg=2.30\n",
            "[767 | 641.10] loss=1.37 avg=2.29\n",
            "[768 | 643.36] loss=1.92 avg=2.28\n",
            "[769 | 645.61] loss=1.20 avg=2.27\n",
            "[770 | 647.86] loss=1.28 avg=2.26\n",
            "[771 | 650.11] loss=1.18 avg=2.25\n",
            "[772 | 652.36] loss=1.40 avg=2.24\n",
            "[773 | 654.61] loss=0.88 avg=2.23\n",
            "[774 | 656.86] loss=2.18 avg=2.23\n",
            "[775 | 659.10] loss=1.51 avg=2.22\n",
            "[776 | 661.35] loss=1.07 avg=2.21\n",
            "[777 | 663.59] loss=1.46 avg=2.20\n",
            "[778 | 665.85] loss=1.43 avg=2.19\n",
            "[779 | 668.09] loss=1.49 avg=2.18\n",
            "[780 | 670.34] loss=1.62 avg=2.18\n",
            "[781 | 672.58] loss=1.37 avg=2.17\n",
            "[782 | 674.83] loss=1.52 avg=2.16\n",
            "[783 | 677.08] loss=1.24 avg=2.15\n",
            "[784 | 679.33] loss=1.48 avg=2.14\n",
            "[785 | 681.57] loss=1.13 avg=2.13\n",
            "[786 | 683.82] loss=1.32 avg=2.12\n",
            "[787 | 686.07] loss=1.27 avg=2.12\n",
            "[788 | 688.31] loss=1.02 avg=2.10\n",
            "[789 | 690.56] loss=1.02 avg=2.09\n",
            "[790 | 692.81] loss=1.30 avg=2.08\n",
            "[791 | 695.06] loss=1.30 avg=2.08\n",
            "[792 | 697.31] loss=1.12 avg=2.07\n",
            "[793 | 699.55] loss=0.97 avg=2.05\n",
            "[794 | 701.81] loss=1.24 avg=2.05\n",
            "[795 | 704.06] loss=0.86 avg=2.03\n",
            "[796 | 706.31] loss=1.38 avg=2.03\n",
            "[797 | 708.55] loss=1.13 avg=2.02\n",
            "[798 | 710.80] loss=1.50 avg=2.01\n",
            "[799 | 713.05] loss=1.00 avg=2.00\n",
            "[800 | 715.30] loss=1.01 avg=1.99\n",
            "======== SAMPLE 1 ========\n",
            " \n",
            "(which is the best expression of the word), is the mode in which\n",
            "mutiny, injury to one's self, and general evil-doings, are,\n",
            "on an average, easily restrained or curtailed. The \"man in\n",
            "battle,\" with his armor and steel, speaks solemnly; there is\n",
            "even a faint ache, in which his teeth are breakfaken, and his soul grows\n",
            "grimly ill. \"You,\" he says, \"you have always stood in the way of\n",
            "powerful foes--and once--woe to you! Gods are ill! Gods are\n",
            "ill! \"--to which the undergrowth grows up with a naked mole of\n",
            "natural history.\n",
            "\n",
            "33. The barbarians are oftener merciless than the Europeans, who\n",
            "think too much as they do everything barbarian, that the way is narrow,\n",
            "that the bow, the cosh, and the breast, and everything that is\n",
            "tender and endurable, that is all-powerful, all-avoiding, all-knowing, all-\n",
            "and-over-all, all goodness and all-sufficient--men who do not know how\n",
            "to bow!--they think, as we do, that there is an All-Gracious,\n",
            "All-Merciful God, who knows how to make a hole in our bow,\n",
            "as a demon does all the other kinds of brass. How easy it is to\n",
            "be a Christian!\n",
            "\n",
            "34. To be barbarians is to be a wilderness, as sheep and beaver are\n",
            "to be useless in the hands of men who are too hard and dangerous\n",
            "to bow. It is barbarism to esteem this mode of life as\n",
            "a mode of existence more strenuous than that of human beings who\n",
            "exercise it should they learn to be untruthful. How easy it is, and how\n",
            "concise, is to think that a god works every day to make us ill; how\n",
            "dangerous is the possibility that he never brings his creatures into\n",
            "this world with him, that there are ever more of them in it than there\n",
            "now are of man. How difficult it is to be a Christian!\n",
            "\n",
            "35. The cruelty and supremacy with which Europeans have viewed Christianity\n",
            "has produced an accumulation of deep wrongsonies, and rendered\n",
            "some one wrongly sick and untrustworthy. How innocent and good-natured\n",
            "we Germans have been in this regard, when we have spoken of Napoleon\n",
            "and the Jews as the \"others\"; how helplessly backward and childish\n",
            "we all Germans nowadays!--these things have always made me sad and\n",
            "bitious; but there is always something sad and heart-breaking in\n",
            "the thought of a Christian man, who thinks himself strong enough, brave enough,\n",
            "and has always sat veryARD nUTHTROFT. How untrustworthy it is in\n",
            "the heart of a Christian man to make known the fact that there are enemies,\n",
            "according to the heart-rendings of such heart-rendings, in the whole series of\n",
            "the Jews, that Christianity is a NIMRITUTE religion, how untruthfulness\n",
            "is a virtue only in this sense. THE RAPE OF\n",
            "BALLEVARDES\n",
            "\n",
            "36. One of the most astonishing things about the whole of the\n",
            "\"Freudian nightmare,\" as I understand it, is that it could not have happened\n",
            "anyhow; the monstrous thing had happened on the very day on which it\n",
            "had taken place; the woman and her four young children had just cried\n",
            "\"O my womb! Oh, who has sinless borne such a pregnancy! And still more\n",
            "whom HA Sir? Who has not sinless borne a son? And still more\n",
            "who has not bore a child? What sort of god would do this! And how\n",
            "contemptible is the lamb for to-day's sake! But gods help\n",
            "others who have sinless borne children! I see in all sorts of\n",
            "vile, infirm, and sinful women--who, like the fainter goats,\n",
            "have borne yet a son, a daughter, or a son--devils too, demons\n",
            "too!--do all this to such a degree, so that there is an\n",
            "inconsistency in their acts, a sort of blindness to the fact, a\n",
            "necessity in their doing what they are obliged to; and the\n",
            "cruelness, necessity, and necessity of all it implies a stupidity that\n",
            "must be suppressed, an abasement and mellification of soul, an\n",
            "incitement to self-destruction that is to be SLEEPING.\n",
            "\n",
            "\n",
            "\n",
            "CHAPTER VIII. THE NATURAL LIES OF THE CATHOLIC DUTY\n",
            "\n",
            "\n",
            "37. The natural enemies of Christianity are demons and\n",
            "badness which neither seek nor receive any counsel whatever. Christianity\n",
            "means to interpret and LAW directly what men mistake, and to\n",
            "perceive almost by accident\n",
            "\n",
            "[801 | 728.26] loss=1.14 avg=1.98\n",
            "[802 | 730.50] loss=0.75 avg=1.97\n",
            "[803 | 732.75] loss=1.18 avg=1.96\n",
            "[804 | 734.99] loss=0.91 avg=1.95\n",
            "[805 | 737.24] loss=0.96 avg=1.94\n",
            "[806 | 739.49] loss=1.06 avg=1.93\n",
            "[807 | 741.74] loss=1.14 avg=1.92\n",
            "[808 | 743.99] loss=1.11 avg=1.91\n",
            "[809 | 746.23] loss=1.07 avg=1.90\n",
            "[810 | 748.48] loss=0.97 avg=1.89\n",
            "[811 | 750.73] loss=0.86 avg=1.88\n",
            "[812 | 752.98] loss=0.91 avg=1.87\n",
            "[813 | 755.23] loss=1.16 avg=1.87\n",
            "[814 | 757.48] loss=0.91 avg=1.86\n",
            "[815 | 759.73] loss=0.85 avg=1.84\n",
            "[816 | 761.98] loss=1.27 avg=1.84\n",
            "[817 | 764.23] loss=1.07 avg=1.83\n",
            "[818 | 766.48] loss=1.08 avg=1.82\n",
            "[819 | 768.72] loss=0.98 avg=1.81\n",
            "[820 | 770.97] loss=1.33 avg=1.81\n",
            "[821 | 773.22] loss=1.10 avg=1.80\n",
            "[822 | 775.48] loss=0.96 avg=1.79\n",
            "[823 | 777.73] loss=1.08 avg=1.79\n",
            "[824 | 779.97] loss=0.88 avg=1.78\n",
            "[825 | 782.22] loss=0.89 avg=1.77\n",
            "[826 | 784.48] loss=0.78 avg=1.76\n",
            "[827 | 786.73] loss=0.89 avg=1.75\n",
            "[828 | 788.98] loss=0.96 avg=1.74\n",
            "[829 | 791.22] loss=0.84 avg=1.73\n",
            "[830 | 793.47] loss=0.99 avg=1.72\n",
            "[831 | 795.72] loss=1.01 avg=1.72\n",
            "[832 | 797.97] loss=0.63 avg=1.70\n",
            "[833 | 800.23] loss=0.68 avg=1.69\n",
            "[834 | 802.48] loss=0.80 avg=1.68\n",
            "[835 | 804.73] loss=0.60 avg=1.67\n",
            "[836 | 806.97] loss=0.61 avg=1.66\n",
            "[837 | 809.22] loss=0.91 avg=1.65\n",
            "[838 | 811.47] loss=1.06 avg=1.65\n",
            "[839 | 813.73] loss=0.73 avg=1.64\n",
            "[840 | 815.98] loss=0.71 avg=1.63\n",
            "[841 | 818.23] loss=0.63 avg=1.62\n",
            "[842 | 820.47] loss=0.73 avg=1.61\n",
            "[843 | 822.72] loss=0.97 avg=1.60\n",
            "[844 | 824.97] loss=0.91 avg=1.60\n",
            "[845 | 827.22] loss=0.60 avg=1.59\n",
            "[846 | 829.47] loss=1.22 avg=1.58\n",
            "[847 | 831.72] loss=0.64 avg=1.57\n",
            "[848 | 833.98] loss=0.72 avg=1.56\n",
            "[849 | 836.23] loss=0.50 avg=1.55\n",
            "[850 | 838.49] loss=0.61 avg=1.54\n",
            "[851 | 840.75] loss=0.59 avg=1.53\n",
            "[852 | 843.00] loss=0.73 avg=1.52\n",
            "[853 | 845.25] loss=0.54 avg=1.51\n",
            "[854 | 847.50] loss=0.66 avg=1.51\n",
            "[855 | 849.77] loss=0.49 avg=1.49\n",
            "[856 | 852.03] loss=0.77 avg=1.49\n",
            "[857 | 854.29] loss=0.67 avg=1.48\n",
            "[858 | 856.53] loss=0.46 avg=1.47\n",
            "[859 | 858.79] loss=0.60 avg=1.46\n",
            "[860 | 861.05] loss=0.66 avg=1.45\n",
            "[861 | 863.31] loss=0.54 avg=1.44\n",
            "[862 | 865.57] loss=0.55 avg=1.43\n",
            "[863 | 867.81] loss=0.84 avg=1.43\n",
            "[864 | 870.06] loss=0.52 avg=1.42\n",
            "[865 | 872.32] loss=0.54 avg=1.41\n",
            "[866 | 874.58] loss=0.63 avg=1.40\n",
            "[867 | 876.82] loss=0.51 avg=1.39\n",
            "[868 | 879.07] loss=0.61 avg=1.38\n",
            "[869 | 881.32] loss=0.53 avg=1.37\n",
            "[870 | 883.57] loss=0.82 avg=1.37\n",
            "[871 | 885.83] loss=0.40 avg=1.36\n",
            "[872 | 888.08] loss=0.70 avg=1.35\n",
            "[873 | 890.33] loss=0.45 avg=1.34\n",
            "[874 | 892.58] loss=0.65 avg=1.34\n",
            "[875 | 894.82] loss=0.41 avg=1.33\n",
            "[876 | 897.07] loss=0.49 avg=1.32\n",
            "[877 | 899.32] loss=0.70 avg=1.31\n",
            "[878 | 901.56] loss=0.58 avg=1.30\n",
            "[879 | 903.81] loss=0.47 avg=1.30\n",
            "[880 | 906.06] loss=0.53 avg=1.29\n",
            "[881 | 908.32] loss=0.38 avg=1.28\n",
            "[882 | 910.58] loss=0.46 avg=1.27\n",
            "[883 | 912.83] loss=0.49 avg=1.26\n",
            "[884 | 915.08] loss=0.66 avg=1.26\n",
            "[885 | 917.32] loss=0.40 avg=1.25\n",
            "[886 | 919.57] loss=0.42 avg=1.24\n",
            "[887 | 921.83] loss=0.39 avg=1.23\n",
            "[888 | 924.08] loss=0.48 avg=1.22\n",
            "[889 | 926.32] loss=0.51 avg=1.22\n",
            "[890 | 928.57] loss=0.47 avg=1.21\n",
            "[891 | 930.82] loss=0.58 avg=1.20\n",
            "[892 | 933.07] loss=0.48 avg=1.19\n",
            "[893 | 935.31] loss=0.46 avg=1.19\n",
            "[894 | 937.56] loss=0.57 avg=1.18\n",
            "[895 | 939.81] loss=0.38 avg=1.17\n",
            "[896 | 942.05] loss=0.45 avg=1.16\n",
            "[897 | 944.31] loss=0.43 avg=1.16\n",
            "[898 | 946.57] loss=0.53 avg=1.15\n",
            "[899 | 948.81] loss=0.60 avg=1.15\n",
            "[900 | 951.06] loss=0.58 avg=1.14\n",
            "======== SAMPLE 1 ========\n",
            " is the first\n",
            "species of \"free will\" to develop into free will\n",
            "which can meet all dangers: \"I\" is the only certainty as to its\n",
            "accumulated amplitude and denial, \"I_Yesterday_was_yesterday.\"...\n",
            "\n",
            "36. In the democratic state of the anarchist, there is, in fact, a\n",
            "dangerous tendency to render the opposite of what you believed or\n",
            "did yourself in as truth. To interpret the acts of one party as those of\n",
            "the other results leads one far astray. In every community of\n",
            "neither\n",
            "blood nor\n",
            "mortals there is always a tendency to assign a determinate destiny to\n",
            "the conduct of the individuals or to forgetfulness entirely. Even\n",
            "weerers of one's neighbor are not placed in a bad condition to\n",
            "participate in the conduct of the neighbor; such persons are, nevertheless,\n",
            " free from the suspicion that we are not those who must assign a\n",
            "certainty to the conduct of the individuals. For this free standing is\n",
            "constant. Everything that is experienced which says \"He did it\" is interpreted\n",
            "in such a way that the individual in turn interprets it\n",
            "under the prevailing taste and custom. As soon as the customary\n",
            "form of inference has been exhausted, the garb of the so-called\n",
            "personal effects, of the usually elaborate and complicated systems of\n",
            "reciprocal enforcers and intruders has ceased to sound; and\n",
            "indeed, a single probability that it did once have a\n",
            "constant and constant interpretation has been preserved totally up to\n",
            "this very day in all the surviving systems of morals and ethics.\n",
            "\n",
            "\n",
            "37\n",
            "\n",
            "=Being Handful in Moral Observances=: and this applies even in the\n",
            "case of the advocates of utilitarianism, who, with the aid of symbolism\n",
            "hitherto, have shown how the act of donating oneself to an end\n",
            "disparages one's strength and power. The man who is honored by the fellow-individual\n",
            "as a benefactor who merits \"doing himself\" in his particular sense--who,\n",
            "with his \"bowent,\" honours \"doing well,\" who, with his emergency,\n",
            "inspires desirability in every one, and commands respect with the art\n",
            "of beneficent obedience, which is \"nevertheless painful\" (as is especially\n",
            "felt in exceptional cases, the case in which justice be exercised);\n",
            "and yet how much weaker is the individual, who, discharging\n",
            "his burden, looks back on himself as an ordinary man, with an admiration\n",
            "for deeds and presumptions, who now deal almost impartially with life.\n",
            "So, too, we are all in many ways, all deficient in wisdom: but if one fully\n",
            "explains away the superficiality of such estimates, one finds\n",
            "many paths to become stronger. The individual, in order to eminence c'est\n",
            "even the wisdom of a community into question, would first of all assign\n",
            "a predetermined value to every deed, so as to have a certain\n",
            "perpetual pace while he is at work, so that he could be fully commended:\n",
            "assessing, making estimates, calculating. With this he could fashion\n",
            "like a man who says, \"I know certain streets and certain seasons,\" or\n",
            "\"I know certain seasons and certain deeds,\" or \"I know certain deeds--owing to\n",
            "regard you in turn.\" Owing to his ingrained habit of estimating at\n",
            "any rate from chance the value of an end to oneself (as the\n",
            "religious formula has it in point of fact), an honest estimate\n",
            "will probably come easily enough), and would be of an unalterable thousand\n",
            "thousands of years. With this method, the individual, after having\n",
            "perished even as a result of an action, may finally say: \"I have discovered the\n",
            "good soil,\" or \"the tendency to meadow,\" or \"the great soil,\" as the\n",
            "beneficial soil and permanent elevation of the all fertile soul. The\n",
            "habit of life begins at last to unfold itself through the entire scheme of\n",
            "the collective experience; the most absolute character is reached by the\n",
            "hunting instinct, which, running upon no surplus of strength, is bent sharply in\n",
            "justice and appropriateness--until at last the all powerful will is\n",
            "permitted to take itself out, and at last the struggle, the all powerful\n",
            "fight begins. Victory is thereby arrivedg at the name, which the\n",
            "habit of life derives from: the habit of the habit of the good\n",
            "feeling. As the free-spirited barbarian who is overcome by his\n",
            "fellow men, so the man of the hundred brilliant and all-round\n",
            "attracts all foes, all dangers, up to a certain pride. Against such\n",
            "fellow-men, he lures into alliances his own type, such as is willing\n",
            "to give him ill repute, such as will not cast the spell. The type is really\n",
            "an exception, and the\n",
            "\n",
            "[901 | 964.38] loss=0.58 avg=1.13\n",
            "[902 | 966.62] loss=0.32 avg=1.13\n",
            "[903 | 968.87] loss=0.34 avg=1.12\n",
            "[904 | 971.12] loss=0.44 avg=1.11\n",
            "[905 | 973.37] loss=0.61 avg=1.11\n",
            "[906 | 975.62] loss=0.54 avg=1.10\n",
            "[907 | 977.87] loss=0.25 avg=1.09\n",
            "[908 | 980.12] loss=0.42 avg=1.08\n",
            "[909 | 982.37] loss=0.34 avg=1.08\n",
            "[910 | 984.62] loss=0.36 avg=1.07\n",
            "[911 | 986.87] loss=0.44 avg=1.06\n",
            "[912 | 989.12] loss=0.49 avg=1.06\n",
            "[913 | 991.37] loss=0.45 avg=1.05\n",
            "[914 | 993.61] loss=0.43 avg=1.04\n",
            "[915 | 995.86] loss=0.44 avg=1.04\n",
            "[916 | 998.11] loss=0.39 avg=1.03\n",
            "[917 | 1000.36] loss=0.34 avg=1.02\n",
            "[918 | 1002.60] loss=0.40 avg=1.02\n",
            "[919 | 1004.86] loss=0.38 avg=1.01\n",
            "[920 | 1007.12] loss=0.29 avg=1.00\n",
            "[921 | 1009.37] loss=0.45 avg=1.00\n",
            "[922 | 1011.61] loss=0.51 avg=0.99\n",
            "[923 | 1013.86] loss=0.32 avg=0.99\n",
            "[924 | 1016.11] loss=0.24 avg=0.98\n",
            "[925 | 1018.38] loss=0.28 avg=0.97\n",
            "[926 | 1020.62] loss=0.22 avg=0.96\n",
            "[927 | 1022.87] loss=0.46 avg=0.96\n",
            "[928 | 1025.12] loss=0.42 avg=0.95\n",
            "[929 | 1027.37] loss=0.30 avg=0.95\n",
            "[930 | 1029.63] loss=0.40 avg=0.94\n",
            "[931 | 1031.88] loss=0.57 avg=0.94\n",
            "[932 | 1034.13] loss=0.57 avg=0.93\n",
            "[933 | 1036.37] loss=0.29 avg=0.93\n",
            "[934 | 1038.63] loss=0.31 avg=0.92\n",
            "[935 | 1040.89] loss=0.36 avg=0.92\n",
            "[936 | 1043.14] loss=0.41 avg=0.91\n",
            "[937 | 1045.39] loss=0.28 avg=0.90\n",
            "[938 | 1047.64] loss=0.28 avg=0.90\n",
            "[939 | 1049.89] loss=0.52 avg=0.89\n",
            "[940 | 1052.14] loss=0.29 avg=0.89\n",
            "[941 | 1054.39] loss=0.28 avg=0.88\n",
            "[942 | 1056.64] loss=0.28 avg=0.88\n",
            "[943 | 1058.89] loss=0.30 avg=0.87\n",
            "[944 | 1061.13] loss=0.22 avg=0.86\n",
            "[945 | 1063.38] loss=0.31 avg=0.86\n",
            "[946 | 1065.63] loss=0.29 avg=0.85\n",
            "[947 | 1067.88] loss=0.33 avg=0.85\n",
            "[948 | 1070.12] loss=0.33 avg=0.84\n",
            "[949 | 1072.37] loss=0.26 avg=0.84\n",
            "[950 | 1074.62] loss=0.41 avg=0.83\n",
            "[951 | 1076.88] loss=0.18 avg=0.83\n",
            "[952 | 1079.13] loss=0.23 avg=0.82\n",
            "[953 | 1081.38] loss=0.31 avg=0.81\n",
            "[954 | 1083.63] loss=0.21 avg=0.81\n",
            "[955 | 1085.88] loss=0.27 avg=0.80\n",
            "[956 | 1088.13] loss=0.29 avg=0.80\n",
            "[957 | 1090.39] loss=0.26 avg=0.79\n",
            "[958 | 1092.64] loss=0.21 avg=0.79\n",
            "[959 | 1094.89] loss=0.42 avg=0.78\n",
            "[960 | 1097.13] loss=0.24 avg=0.78\n",
            "[961 | 1099.38] loss=0.19 avg=0.77\n",
            "[962 | 1101.64] loss=0.23 avg=0.77\n",
            "[963 | 1103.90] loss=0.23 avg=0.76\n",
            "[964 | 1106.14] loss=0.22 avg=0.75\n",
            "[965 | 1108.39] loss=0.21 avg=0.75\n",
            "[966 | 1110.64] loss=0.21 avg=0.74\n",
            "[967 | 1112.89] loss=0.21 avg=0.74\n",
            "[968 | 1115.14] loss=0.23 avg=0.73\n",
            "[969 | 1117.39] loss=0.25 avg=0.73\n",
            "[970 | 1119.64] loss=0.28 avg=0.72\n",
            "[971 | 1121.89] loss=0.25 avg=0.72\n",
            "[972 | 1124.14] loss=0.31 avg=0.71\n",
            "[973 | 1126.40] loss=0.61 avg=0.71\n",
            "[974 | 1128.66] loss=0.23 avg=0.71\n",
            "[975 | 1130.90] loss=0.29 avg=0.70\n",
            "[976 | 1133.15] loss=0.19 avg=0.70\n",
            "[977 | 1135.40] loss=0.19 avg=0.69\n",
            "[978 | 1137.64] loss=0.21 avg=0.69\n",
            "[979 | 1139.89] loss=0.20 avg=0.68\n",
            "[980 | 1142.14] loss=0.42 avg=0.68\n",
            "[981 | 1144.39] loss=0.27 avg=0.68\n",
            "[982 | 1146.64] loss=0.24 avg=0.67\n",
            "[983 | 1148.89] loss=0.24 avg=0.67\n",
            "[984 | 1151.15] loss=0.19 avg=0.66\n",
            "[985 | 1153.40] loss=0.40 avg=0.66\n",
            "[986 | 1155.65] loss=0.26 avg=0.66\n",
            "[987 | 1157.90] loss=0.21 avg=0.65\n",
            "[988 | 1160.14] loss=0.19 avg=0.65\n",
            "[989 | 1162.39] loss=0.21 avg=0.64\n",
            "[990 | 1164.64] loss=0.24 avg=0.64\n",
            "[991 | 1166.88] loss=0.16 avg=0.63\n",
            "[992 | 1169.13] loss=0.30 avg=0.63\n",
            "[993 | 1171.37] loss=0.18 avg=0.63\n",
            "[994 | 1173.62] loss=0.19 avg=0.62\n",
            "[995 | 1175.87] loss=0.25 avg=0.62\n",
            "[996 | 1178.11] loss=0.20 avg=0.61\n",
            "[997 | 1180.35] loss=0.25 avg=0.61\n",
            "[998 | 1182.60] loss=0.21 avg=0.61\n",
            "[999 | 1184.83] loss=0.23 avg=0.60\n",
            "[1000 | 1187.08] loss=0.22 avg=0.60\n",
            "Saving checkpoint/run1/model-1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Text generation"
      ],
      "metadata": {
        "id": "bUagiJzBTeoL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prefix = \"What's the recipe for pasta?\""
      ],
      "metadata": {
        "id": "qzTK7bdIPeOY"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpt2.generate(sess, prefix=prefix, length=150)"
      ],
      "metadata": {
        "id": "ZCaaNXR7kI9I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd693466-243c-440e-cc7a-f0a4f4503baa"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What's the recipe for pasta?--grin, griddle, gander, gander, gander--?\"\n",
            "\n",
            "Thereupon the cook lifted the lid and there was quite an excitement of the stomach--indeed, my stomach was almost on fire. I found myself thinking quite\n",
            "without fluency about the dangers of food which were dangerous only to\n",
            "warmth and pressure. I was thinking rather of the radiant stink of the finest\n",
            "pest control cells, the unmistakable stink of the finest insect control\n",
            "cells, the constantly disturbing voice of the Cagliostro and of the secret nerve apparatus\n",
            "in place of the sensation of touch, the extraordinarily hard, almost\n",
            "woven, almost half-iced, delicacy,--that pretty much all food,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Saving model to Google Drive (optional)"
      ],
      "metadata": {
        "id": "zlM6aQYZSccl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYXmOFl5Bjhv",
        "outputId": "564ebb74-2ba5-403f-dc4b-c36ce1478d52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpt2.copy_checkpoint_to_gdrive(run_name='run1')"
      ],
      "metadata": {
        "id": "3RUjr4_ZluKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can find more texts e.g. on:\n",
        "https://www.gutenberg.org/cache/epub/1597/pg1597.txt\n",
        "</br></br>\n",
        "You can download them to Colab using code similar to the ones below."
      ],
      "metadata": {
        "id": "OUhaGg_uS6o5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!wget https://www.gutenberg.org/cache/epub/1597/pg1597.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7K9X3K8TEwj",
        "outputId": "d0760c42-a0e4-4dcf-b7cc-ca98aaffa2b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-21 14:49:16--  https://www.gutenberg.org/cache/epub/1597/pg1597.txt\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 329071 (321K) [text/plain]\n",
            "Saving to: â€˜pg1597.txtâ€™\n",
            "\n",
            "pg1597.txt          100%[===================>] 321.36K   800KB/s    in 0.4s    \n",
            "\n",
            "2023-03-21 14:49:22 (800 KB/s) - â€˜pg1597.txtâ€™ saved [329071/329071]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!wget https://www.gutenberg.org/files/98/98-0.txt"
      ],
      "metadata": {
        "id": "HYL0wij2m4Gf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42bf360b-ce90-4a36-d434-44820124b877"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-02-22 13:25:10--  https://www.gutenberg.org/files/98/98-0.txt\n",
            "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
            "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 807231 (788K) [text/plain]\n",
            "Saving to: â€˜98-0.txtâ€™\n",
            "\n",
            "98-0.txt            100%[===================>] 788.31K   718KB/s    in 1.1s    \n",
            "\n",
            "2023-02-22 13:25:12 (718 KB/s) - â€˜98-0.txtâ€™ saved [807231/807231]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#https://github.com/matt-dray/tng-stardate/tree/master/data/scripts"
      ],
      "metadata": {
        "id": "VClsbkgRxYvR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}